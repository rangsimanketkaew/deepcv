{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DeepCV DeepCV is a deep learning framework for training a machine learning model on molecular representations to learn collective variables (CVs).","title":"Introduction"},{"location":"#deepcv","text":"DeepCV is a deep learning framework for training a machine learning model on molecular representations to learn collective variables (CVs).","title":"DeepCV"},{"location":"citing/","text":"Citing DeepCV If you use DeepCV or find it useful for your researcy, please cite this: Rangsiman Ketkaew, Fabrizio Creazzo, and Sandra Luber J. Phys. Chem. Lett. 2022, 13, XXX, 1797\u20131805 https://doi.org/10.1021/acs.jpclett.1c04004 Bibtex @article { Ketkaew2022 , doi = {10.1021/acs.jpclett.1c04004} , url = {https://doi.org/10.1021/acs.jpclett.1c04004} , year = {2022} , month = feb , publisher = {American Chemical Society ({ACS})} , pages = {1797--1805} , author = {Rangsiman Ketkaew and Fabrizio Creazzo and Sandra Luber} , title = {Machine Learning-Assisted Discovery of Hidden States in Expanded Free Energy Space} , journal = {The Journal of Physical Chemistry Letters} }","title":"Citing"},{"location":"citing/#citing-deepcv","text":"If you use DeepCV or find it useful for your researcy, please cite this: Rangsiman Ketkaew, Fabrizio Creazzo, and Sandra Luber J. Phys. Chem. Lett. 2022, 13, XXX, 1797\u20131805 https://doi.org/10.1021/acs.jpclett.1c04004 Bibtex @article { Ketkaew2022 , doi = {10.1021/acs.jpclett.1c04004} , url = {https://doi.org/10.1021/acs.jpclett.1c04004} , year = {2022} , month = feb , publisher = {American Chemical Society ({ACS})} , pages = {1797--1805} , author = {Rangsiman Ketkaew and Fabrizio Creazzo and Sandra Luber} , title = {Machine Learning-Assisted Discovery of Hidden States in Expanded Free Energy Space} , journal = {The Journal of Physical Chemistry Letters} }","title":"Citing DeepCV"},{"location":"installation/env-setup/","text":"Install DeepCV on your local machine Clone DeepCV repo or download a tarball to your local machine For git, clone with SSH is recommended: git clone git@gitlab.uzh.ch:lubergroup/deepcv.git Environment setup & install dependencies cd deepcv/ conda create -n deepcv python == 3 .8 conda activate deepcv conda update --all -y","title":"Environment setup"},{"location":"installation/env-setup/#install-deepcv-on-your-local-machine","text":"","title":"Install DeepCV on your local machine"},{"location":"installation/env-setup/#clone-deepcv-repo-or-download-a-tarball-to-your-local-machine","text":"For git, clone with SSH is recommended: git clone git@gitlab.uzh.ch:lubergroup/deepcv.git","title":"Clone DeepCV repo or download a tarball to your local machine"},{"location":"installation/env-setup/#environment-setup-install-dependencies","text":"cd deepcv/ conda create -n deepcv python == 3 .8 conda activate deepcv conda update --all -y","title":"Environment setup &amp; install dependencies"},{"location":"installation/install/","text":"Installing DeepCV Installing dependencies pip install -r requirements.txt # or conda install --file requirements.txt Installing DeepCV cd deepcv/ conda create -n deepcv python == 3 .8 conda activate deepcv conda update --all -y pip install -r requirements.txt # or conda install --file requirements.txt Optional: for C++ API g++ -c -I/path/to/plumed/src/ -o deepcv.o deepcv.cpp g++ -shared -fPIC -o deepcv.so deepcv.o or just type make Check if GPU is available for TF Execute this script to check if TensorFlow 2.x can see GPU. import tensorflow as tf assert tf . test . is_gpu_available (), \"TF can't see GPU on this machine.\" assert tf . test . is_built_with_cuda (), \"TF was not built with CUDA.\"","title":"Installing"},{"location":"installation/install/#installing-deepcv","text":"","title":"Installing DeepCV"},{"location":"installation/install/#installing-dependencies","text":"pip install -r requirements.txt # or conda install --file requirements.txt","title":"Installing dependencies"},{"location":"installation/install/#installing-deepcv_1","text":"cd deepcv/ conda create -n deepcv python == 3 .8 conda activate deepcv conda update --all -y pip install -r requirements.txt # or conda install --file requirements.txt Optional: for C++ API g++ -c -I/path/to/plumed/src/ -o deepcv.o deepcv.cpp g++ -shared -fPIC -o deepcv.so deepcv.o or just type make","title":"Installing DeepCV"},{"location":"installation/install/#check-if-gpu-is-available-for-tf","text":"Execute this script to check if TensorFlow 2.x can see GPU. import tensorflow as tf assert tf . test . is_gpu_available (), \"TF can't see GPU on this machine.\" assert tf . test . is_built_with_cuda (), \"TF was not built with CUDA.\"","title":"Check if GPU is available for TF"},{"location":"installation/overview/","text":"Overview of Installing DeepCV DeepCV can be installed via pip , a Python package manager, or conda , another manager framework. However, before doing that, one should create a separate working space for DeepCV to prevent redundancy of the package dependencies.","title":"Overview"},{"location":"installation/overview/#overview-of-installing-deepcv","text":"DeepCV can be installed via pip , a Python package manager, or conda , another manager framework. However, before doing that, one should create a separate working space for DeepCV to prevent redundancy of the package dependencies.","title":"Overview of Installing DeepCV"},{"location":"usage/fes-analysis/","text":"Free energy surface analysis Convergence of FES Committor analysis (example) import numpy as np committor = np . where ( dat [ 1 ] > 1.65 , 1 , ( np . where ( dat [ 1 ] < 1.45 , - 1 , 0 ))) # R3 Visualizing DAENN model's latent space Check au_visual.py script. It can be used to visualize feature representations (latent space) of while training a model.","title":"Free energy surface"},{"location":"usage/fes-analysis/#free-energy-surface-analysis","text":"","title":"Free energy surface analysis"},{"location":"usage/fes-analysis/#convergence-of-fes","text":"Committor analysis (example) import numpy as np committor = np . where ( dat [ 1 ] > 1.65 , 1 , ( np . where ( dat [ 1 ] < 1.45 , - 1 , 0 ))) # R3","title":"Convergence of FES"},{"location":"usage/fes-analysis/#visualizing-daenn-models-latent-space","text":"Check au_visual.py script. It can be used to visualize feature representations (latent space) of while training a model.","title":"Visualizing DAENN model's latent space"},{"location":"usage/metad-sim/","text":"Running Metadynamics simulation Create a PLUMED input file Once the training is complete, you can use deecv2plumed script to generate the PLUMED input file. It takes the same input as you used for daenn.py . It will automatically extract the weight and bias from the model and print out the file. $ python main.py deepcv2plumed -i input/input_ae_DA.json -n 16 -o plumed-NN.dat >>> Plumed data have been written successfully to 'plumed-NN.dat' >>> In order to run metadynamics using CP2K & PLUMED, specify the following input deck in CP2K input: # Import PLUMED input file & MOTION & FREE_ENERGY & METADYN USE_PLUMED .TRUE. PLUMED_INPUT_FILE plumed-NN.dat & END METADYN & END FREE_ENERGY %END MOTION Test plumed input file This step is to check if a generated PLUMED input file works or not. You can use PLUMED driver to run a trial test on a 1-frame simple Diels-Alder trajectory. $ plumed driver --ixyz reactant_DA_water_100atoms.xyz --plumed plumed-NN.dat $ tree . \u251c\u2500\u2500 bias.log \u251c\u2500\u2500 COLVAR.log \u251c\u2500\u2500 HILLS \u251c\u2500\u2500 input_plumed.log \u251c\u2500\u2500 layer1.log \u251c\u2500\u2500 layer2.log \u251c\u2500\u2500 layer3.log \u251c\u2500\u2500 plumed-NN.dat \u2514\u2500\u2500 reactant_DA_water_100atoms.xyz Prepare CP2K input files for performing metadynamics simulation Prepare all necessary files and run metadynamics simulation using CP2K. $ tree . \u251c\u2500\u2500 dftd3.dat # DFT-D3 parameter \u251c\u2500\u2500 MetaD.inp # CP2K input file \u251c\u2500\u2500 plumed-NN.dat # PLUMED input file \u251c\u2500\u2500 reactant_DA_water_100atoms.xyz # Coordinate file \u251c\u2500\u2500 run_script.sh # SLURM script \u2514\u2500\u2500 xTB_parameters # xTB parameter needed only you want to use extended Tight binding Step 4: Submit job on Piz Daint Prepare a SLURM input file, for example: #!/bin/bash -l # # ----- SLURM JOB SUBMIT SCRIPT ----- #SBATCH --export=ALL #SBATCH --error=slurm.%J.err #SBATCH --output=slurm.%J.out #SBATCH --exclusive ################ CHANGE this section (begin) ########################## # -- job info -- #SBATCH --account=s1036 #SBATCH --partition=normal #SBATCH --time=24:00:00 # -- number of nodes and CPU usage -- #SBATCH --nodes=48 # of nodes (default = 1) #SBATCH --ntasks-per-node=12 # of MPI tasks/node (default = 36) #SBATCH --cpus-per-task=1 # of OMP threads/task (default = 1) #SBATCH --ntasks-per-core=1 # HT (default = 1, HyperThreads = 2) #SBATCH --constraint=gpu # CPU partition ###########CP2K######################################################### # -- the program and input file (basename) -- EXE = \"/full/path/to/executable/cp2k.psmp\" SLURM_NTASKS_PER_NODE = 12 # you can also define/declare/load modules and packages needed for CP2K and PLUMED here ################ NOTHING to be changed here ############################ PROJECT = ${ SLURM_JOB_NAME } PROJECT = $( basename $PROJECT .inp ) INP = \" ${ PROJECT } .inp\" OUT = \" ${ PROJECT } .out\" INPDIR = \" $PWD \" PROJECTDIR = \" ${ INPDIR //scratch \\/ snx3000 /project/s745 } \" SLURM_NTASKS_PER_NODE = 12 echo ' --------------------------------------------------------------' echo ' | --- RUNNING JOB --- |' echo ' --------------------------------------------------------------' echo \" ${ SLURM_NTASK_PER_CORE } \" # stop if maximum number of processes per node is exceeded if [ ${ SLURM_NTASKS_PER_CORE } -eq 1 ] ; then if [ $(( ${ SLURM_NTASKS_PER_NODE } * ${ SLURM_CPUS_PER_TASK } )) -gt 36 ] ; then echo 'Number of processes per node is too large! (STOPPING)' exit 1 fi else if [ $(( ${ SLURM_NTASKS_PER_NODE } * ${ SLURM_CPUS_PER_TASK } )) -gt 72 ] ; then echo 'Number of processes per node is too large! (STOPPING)' exit 1 fi fi # build SRUN command srun_options = \"\\ --exclusive \\ --bcast=/tmp/ ${ USER } \\ --nodes= ${ SLURM_NNODES } \\ --ntasks= ${ SLURM_NTASKS } \\ --ntasks-per-node= ${ SLURM_NTASKS_PER_NODE } \\ --cpus-per-task= ${ SLURM_CPUS_PER_TASK } \\ --ntasks-per-core= ${ SLURM_NTASKS_PER_CORE } \" if [ ${ SLURM_CPUS_PER_TASK } -gt 1 ] ; then export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK } else export OMP_NUM_THREADS = 1 fi srun_command = \"/usr/bin/time -p srun ${ srun_options } \" # print some informations nnodes = ${ SLURM_NNODES } nranks_per_node = ${ SLURM_NTASKS_PER_NODE } nranks = $(( ${ nranks_per_node } * ${ nnodes } )) nthreads_per_rank = ${ SLURM_CPUS_PER_TASK } nthreads = $(( ${ nthreads_per_rank } * ${ nranks } )) echo \"SRUN-command: ${ srun_command } \" echo \"JOB-config: nodes= ${ nnodes } ranks/nodes= ${ nranks_per_node } threads/rank= ${ nthreads_per_rank } \" echo \"JOB-total: nodes= ${ nnodes } ranks= ${ nranks } threads= ${ nthreads } \" # run the program ${ srun_command } ${ EXE } -i ${ INP } -o ${ OUT } echo ' --------------------------------------------------------------' echo ' | --- DONE --- |' echo ' --------------------------------------------------------------' exit 0 and then submit the job $ sbatch -J MetaD.inp run_script.sh","title":"Metadynamics"},{"location":"usage/metad-sim/#running-metadynamics-simulation","text":"","title":"Running Metadynamics simulation"},{"location":"usage/metad-sim/#create-a-plumed-input-file","text":"Once the training is complete, you can use deecv2plumed script to generate the PLUMED input file. It takes the same input as you used for daenn.py . It will automatically extract the weight and bias from the model and print out the file. $ python main.py deepcv2plumed -i input/input_ae_DA.json -n 16 -o plumed-NN.dat >>> Plumed data have been written successfully to 'plumed-NN.dat' >>> In order to run metadynamics using CP2K & PLUMED, specify the following input deck in CP2K input: # Import PLUMED input file & MOTION & FREE_ENERGY & METADYN USE_PLUMED .TRUE. PLUMED_INPUT_FILE plumed-NN.dat & END METADYN & END FREE_ENERGY %END MOTION","title":"Create a PLUMED input file"},{"location":"usage/metad-sim/#test-plumed-input-file","text":"This step is to check if a generated PLUMED input file works or not. You can use PLUMED driver to run a trial test on a 1-frame simple Diels-Alder trajectory. $ plumed driver --ixyz reactant_DA_water_100atoms.xyz --plumed plumed-NN.dat $ tree . \u251c\u2500\u2500 bias.log \u251c\u2500\u2500 COLVAR.log \u251c\u2500\u2500 HILLS \u251c\u2500\u2500 input_plumed.log \u251c\u2500\u2500 layer1.log \u251c\u2500\u2500 layer2.log \u251c\u2500\u2500 layer3.log \u251c\u2500\u2500 plumed-NN.dat \u2514\u2500\u2500 reactant_DA_water_100atoms.xyz","title":"Test plumed input file"},{"location":"usage/metad-sim/#prepare-cp2k-input-files-for-performing-metadynamics-simulation","text":"Prepare all necessary files and run metadynamics simulation using CP2K. $ tree . \u251c\u2500\u2500 dftd3.dat # DFT-D3 parameter \u251c\u2500\u2500 MetaD.inp # CP2K input file \u251c\u2500\u2500 plumed-NN.dat # PLUMED input file \u251c\u2500\u2500 reactant_DA_water_100atoms.xyz # Coordinate file \u251c\u2500\u2500 run_script.sh # SLURM script \u2514\u2500\u2500 xTB_parameters # xTB parameter needed only you want to use extended Tight binding","title":"Prepare CP2K input files for performing metadynamics simulation"},{"location":"usage/metad-sim/#step-4-submit-job-on-piz-daint","text":"Prepare a SLURM input file, for example: #!/bin/bash -l # # ----- SLURM JOB SUBMIT SCRIPT ----- #SBATCH --export=ALL #SBATCH --error=slurm.%J.err #SBATCH --output=slurm.%J.out #SBATCH --exclusive ################ CHANGE this section (begin) ########################## # -- job info -- #SBATCH --account=s1036 #SBATCH --partition=normal #SBATCH --time=24:00:00 # -- number of nodes and CPU usage -- #SBATCH --nodes=48 # of nodes (default = 1) #SBATCH --ntasks-per-node=12 # of MPI tasks/node (default = 36) #SBATCH --cpus-per-task=1 # of OMP threads/task (default = 1) #SBATCH --ntasks-per-core=1 # HT (default = 1, HyperThreads = 2) #SBATCH --constraint=gpu # CPU partition ###########CP2K######################################################### # -- the program and input file (basename) -- EXE = \"/full/path/to/executable/cp2k.psmp\" SLURM_NTASKS_PER_NODE = 12 # you can also define/declare/load modules and packages needed for CP2K and PLUMED here ################ NOTHING to be changed here ############################ PROJECT = ${ SLURM_JOB_NAME } PROJECT = $( basename $PROJECT .inp ) INP = \" ${ PROJECT } .inp\" OUT = \" ${ PROJECT } .out\" INPDIR = \" $PWD \" PROJECTDIR = \" ${ INPDIR //scratch \\/ snx3000 /project/s745 } \" SLURM_NTASKS_PER_NODE = 12 echo ' --------------------------------------------------------------' echo ' | --- RUNNING JOB --- |' echo ' --------------------------------------------------------------' echo \" ${ SLURM_NTASK_PER_CORE } \" # stop if maximum number of processes per node is exceeded if [ ${ SLURM_NTASKS_PER_CORE } -eq 1 ] ; then if [ $(( ${ SLURM_NTASKS_PER_NODE } * ${ SLURM_CPUS_PER_TASK } )) -gt 36 ] ; then echo 'Number of processes per node is too large! (STOPPING)' exit 1 fi else if [ $(( ${ SLURM_NTASKS_PER_NODE } * ${ SLURM_CPUS_PER_TASK } )) -gt 72 ] ; then echo 'Number of processes per node is too large! (STOPPING)' exit 1 fi fi # build SRUN command srun_options = \"\\ --exclusive \\ --bcast=/tmp/ ${ USER } \\ --nodes= ${ SLURM_NNODES } \\ --ntasks= ${ SLURM_NTASKS } \\ --ntasks-per-node= ${ SLURM_NTASKS_PER_NODE } \\ --cpus-per-task= ${ SLURM_CPUS_PER_TASK } \\ --ntasks-per-core= ${ SLURM_NTASKS_PER_CORE } \" if [ ${ SLURM_CPUS_PER_TASK } -gt 1 ] ; then export OMP_NUM_THREADS = ${ SLURM_CPUS_PER_TASK } else export OMP_NUM_THREADS = 1 fi srun_command = \"/usr/bin/time -p srun ${ srun_options } \" # print some informations nnodes = ${ SLURM_NNODES } nranks_per_node = ${ SLURM_NTASKS_PER_NODE } nranks = $(( ${ nranks_per_node } * ${ nnodes } )) nthreads_per_rank = ${ SLURM_CPUS_PER_TASK } nthreads = $(( ${ nthreads_per_rank } * ${ nranks } )) echo \"SRUN-command: ${ srun_command } \" echo \"JOB-config: nodes= ${ nnodes } ranks/nodes= ${ nranks_per_node } threads/rank= ${ nthreads_per_rank } \" echo \"JOB-total: nodes= ${ nnodes } ranks= ${ nranks } threads= ${ nthreads } \" # run the program ${ srun_command } ${ EXE } -i ${ INP } -o ${ OUT } echo ' --------------------------------------------------------------' echo ' | --- DONE --- |' echo ' --------------------------------------------------------------' exit 0 and then submit the job $ sbatch -J MetaD.inp run_script.sh","title":"Step 4: Submit job on Piz Daint"},{"location":"usage/overview/","text":"Using DeepCV","title":"Overview"},{"location":"usage/overview/#using-deepcv","text":"","title":"Using DeepCV"},{"location":"usage/prepare-dataset/","text":"Preparing data set First step prior to calculation of representation is to generate a trajectory of reactant conformers. This can be done by using (unbiased) molecular dynamics (MD) simulation to generate a trajectory of the system of interest. To do so, we recommend general MD packages, e.g., CP2K or GROMACS, because they have an interface with PLUMED, which is a plugin for running metadynamics simulation. $ ls traj.xyz Split a trajectory file into smaller files It is often that a trajectory file (.xyz) is so large. So we can split it into multiple smaller files using split command in Linux. For example, my trajectory contains 4000 structures with 50 atoms each. In .xyz file, each structure has 1 line denoting total number of atoms in a molecule, 1 comment line, and 50 lines of coordinates, resulting in total of 52 lines. If we want to split every 20 th structure, we have to define the --lines with 1040 (52x20). Other options can also be used. $ split --lines = 1040 --numeric-suffixes = 001 --suffix-length = 3 traj.xyz traj-partial- --additional-suffix = .xyz $ ls traj-partial-001.xyz traj-partial-002.xyz traj-partial-003.xyz ... traj-partial-100.xyz Calculate molecular representations and generate input files (dataset) for neural network 1. Z-matrix (internal coordinate) $ deepcv/src/tools/calc_rep.py --input traj-partial-001.xyz --rep zmat --save Converting text data to NumPy array... Shape of NumPy array: ( 50 , 100 , 3 ) Calculate internal coordinates of all structures 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 50 /50 [ 00 :00< 00 :00, 141 .18it/s ] Check files $ ls *zmat* traj-partial-001_zmat_strc_1.npz traj-partial-001_zmat_strc_2.npz traj-partial-001_zmat_strc_3.npz ... 2. SPRINT and xSPRINT $ deepcv/src/tools/calc_rep.py --input traj-partial-001.xyz --rep sprint --save Converting text data to NumPy array... Shape of NumPy array: ( 50 , 100 , 3 ) Calculate xSPRINT coordinates and sorted atom index 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 50 /50 [ 00 :03< 00 :00, 14 .00it/s ] And you can loop over all files, e.g., $ for i in traj-partial-*.xyz ; do echo $i ; deepcv/src/tools/calc_rep.py --input $i --rep zmat --save ; done Merge multiple npz files into one npz file In this step, we will merge all individual npz files for the same kind of distance, angle, and torsion (separately). $ deepcv/src/helpers/stack_array.py -i traj-partial-*_zmat_strc_* -k dist 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 49 /49 [ 00 :00< 00 :00, 376 .17it/s ] Shape of output NumPy array ( after stacking ) : ( 50 , 100 ) Optional: Convert .xyz to .npz You can use a script called xyz2npz.py to convert .xyz file to NumPy's compressed file formats (.npz). It will also save a new file with a prefix of the key in npz (default key is coord ). $ deepcv/src/helpers/xyz2arr.py -i traj-partial-001.xyz $ ls *.npz traj-partial-001.npz ## and use for loop for automated task $ for i in traj-partial-*.xyz ; do echo $i ; deepcv/src/helpers/xyz2arr.py -i $i ; done ... output snipped out ... $ ls *.npz traj-partial-001_coord.npz traj-partial-002_coord.npz traj-partial-003_coord.npz ... traj-partial-100_coord.npz then run Python's command prompt to check if everything about saved npz goes well $ python >>> import numpy as np >>> a = np.load ( \"traj-partial-001_coord.npz\" ) >>> a.files [ 'coord' ] >>> a [ 'coord' ] .shape ( 20 , 50 , 3 )","title":"Datasets"},{"location":"usage/prepare-dataset/#preparing-data-set","text":"First step prior to calculation of representation is to generate a trajectory of reactant conformers. This can be done by using (unbiased) molecular dynamics (MD) simulation to generate a trajectory of the system of interest. To do so, we recommend general MD packages, e.g., CP2K or GROMACS, because they have an interface with PLUMED, which is a plugin for running metadynamics simulation. $ ls traj.xyz","title":"Preparing data set"},{"location":"usage/prepare-dataset/#split-a-trajectory-file-into-smaller-files","text":"It is often that a trajectory file (.xyz) is so large. So we can split it into multiple smaller files using split command in Linux. For example, my trajectory contains 4000 structures with 50 atoms each. In .xyz file, each structure has 1 line denoting total number of atoms in a molecule, 1 comment line, and 50 lines of coordinates, resulting in total of 52 lines. If we want to split every 20 th structure, we have to define the --lines with 1040 (52x20). Other options can also be used. $ split --lines = 1040 --numeric-suffixes = 001 --suffix-length = 3 traj.xyz traj-partial- --additional-suffix = .xyz $ ls traj-partial-001.xyz traj-partial-002.xyz traj-partial-003.xyz ... traj-partial-100.xyz","title":"Split a trajectory file into smaller files"},{"location":"usage/prepare-dataset/#calculate-molecular-representations-and-generate-input-files-dataset-for-neural-network","text":"","title":"Calculate molecular representations and generate input files (dataset) for neural network"},{"location":"usage/prepare-dataset/#1-z-matrix-internal-coordinate","text":"$ deepcv/src/tools/calc_rep.py --input traj-partial-001.xyz --rep zmat --save Converting text data to NumPy array... Shape of NumPy array: ( 50 , 100 , 3 ) Calculate internal coordinates of all structures 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 50 /50 [ 00 :00< 00 :00, 141 .18it/s ] Check files $ ls *zmat* traj-partial-001_zmat_strc_1.npz traj-partial-001_zmat_strc_2.npz traj-partial-001_zmat_strc_3.npz ...","title":"1. Z-matrix (internal coordinate)"},{"location":"usage/prepare-dataset/#2-sprint-and-xsprint","text":"$ deepcv/src/tools/calc_rep.py --input traj-partial-001.xyz --rep sprint --save Converting text data to NumPy array... Shape of NumPy array: ( 50 , 100 , 3 ) Calculate xSPRINT coordinates and sorted atom index 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 50 /50 [ 00 :03< 00 :00, 14 .00it/s ] And you can loop over all files, e.g., $ for i in traj-partial-*.xyz ; do echo $i ; deepcv/src/tools/calc_rep.py --input $i --rep zmat --save ; done","title":"2. SPRINT and xSPRINT"},{"location":"usage/prepare-dataset/#merge-multiple-npz-files-into-one-npz-file","text":"In this step, we will merge all individual npz files for the same kind of distance, angle, and torsion (separately). $ deepcv/src/helpers/stack_array.py -i traj-partial-*_zmat_strc_* -k dist 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 49 /49 [ 00 :00< 00 :00, 376 .17it/s ] Shape of output NumPy array ( after stacking ) : ( 50 , 100 )","title":"Merge multiple npz files into one npz file"},{"location":"usage/prepare-dataset/#optional-convert-xyz-to-npz","text":"You can use a script called xyz2npz.py to convert .xyz file to NumPy's compressed file formats (.npz). It will also save a new file with a prefix of the key in npz (default key is coord ). $ deepcv/src/helpers/xyz2arr.py -i traj-partial-001.xyz $ ls *.npz traj-partial-001.npz ## and use for loop for automated task $ for i in traj-partial-*.xyz ; do echo $i ; deepcv/src/helpers/xyz2arr.py -i $i ; done ... output snipped out ... $ ls *.npz traj-partial-001_coord.npz traj-partial-002_coord.npz traj-partial-003_coord.npz ... traj-partial-100_coord.npz then run Python's command prompt to check if everything about saved npz goes well $ python >>> import numpy as np >>> a = np.load ( \"traj-partial-001_coord.npz\" ) >>> a.files [ 'coord' ] >>> a [ 'coord' ] .shape ( 20 , 50 , 3 )","title":"Optional: Convert .xyz to .npz"},{"location":"usage/states-analysis/","text":"Metastable states sampling analysis Visualizing the latent space Check au_visual.py script. It can be used to visualize feature representations (latent space) of while training a model.","title":"Metastable states"},{"location":"usage/states-analysis/#metastable-states-sampling-analysis","text":"","title":"Metastable states sampling analysis"},{"location":"usage/states-analysis/#visualizing-the-latent-space","text":"Check au_visual.py script. It can be used to visualize feature representations (latent space) of while training a model.","title":"Visualizing the latent space"},{"location":"usage/training-model/","text":"Training a DAENN model Note: the current version of DeepCV accepts only datasets that are in NumPy's compressed file formats (.npz). Prepare input file for Diels-Alder reaction DeepCV's input file must be in a JSON file format (dictionary-like). An example of DAENN input in inputs/ folder shows the configuration for training a model using DAENN with five hidden layers. The first three hidden layers contain two encoded layers and one latent encoded layer (middle layer). The rest layers are two decoded layers for reconstruction. On the other hand, the size of two hidden layers that are opposite of each other, e.g., input and output layers, 1 st and 5 th hidden layers, must be the same. Call DeepCV's module All DeepCV's modules can be called via main.py API script. $ cd deepcv/src/ $ python main.py ------------------------------------------------ DeepCV : Deep Learning for Collective Variables ------------------------------------------------- version 1 .0 : February 2022 University of Zurich, Switzerland https://gitlab.uzh.ch/lubergroup/deepcv Module Description ------------- ---------------------------------------------- calc_rep Calculate molecular representation gen_input Neural network input generator single_train Single-data fully-connected neural network multi_train Multi-data fully-connected neural network daenn Deep autoencoder neural network gan_train Training generative adversarial network ( GAN ) gan_predict Generating samples using trained GAN deepcv2plumed Create PLUMED input file analyze_FES FES validation analyze_model DAENN model analysis and parameters extraction explor_abi Calculate exploration ability For more detail, please review 'README' in the repository. Train model Execute the main.py with argument daenn , like below. Then it will start to train the model. The training time depends on the size of the dataset and networks, the number of epochs, etc. $ python main.py daenn -i input/input_ae_DA.json ============================== Program started ============================== Project: Demo === Shape of dataset before splitting === >>> 1 . Dataset: ( 99000 , 15 ) >>> 2 . Dataset: ( 99000 , 14 ) >>> 3 . Dataset: ( 99000 , 13 ) === Shape of dataset after splitting === >>> 1 . Train: ( 79200 , 15 ) & Test: ( 19800 , 15 ) >>> 2 . Train: ( 79200 , 14 ) & Test: ( 19800 , 14 ) >>> 3 . Train: ( 79200 , 13 ) & Test: ( 19800 , 13 ) ... ... ... 1 /1 [==============================] - ETA: 0s - loss: 0 .1283 - out_1_loss: 0 .1268 - out_2_loss: 0 .0154 - out_1_mse: 4 .0202e-04 - out_2_mse: 61 /1\u2588 ETA: 00 :00s - loss: 0 .1283 - out_1_loss: 0 .1268 - out_2_loss: 0 .0154 - out_1_mse: 0 .0004 - out_2_mse: 67 .5435 - val_loss: 0 .1176 - val_ou1/1 [==============================] - 0s 21ms/step - loss: 0 .1283 - out_1_loss: 0 .1268 - out_2_loss: 0 .0154 - out_1_mse: 4 .0202e-04 - out_2_mse: 67 .5435 - val_loss: 0 .1176 - val_out_1_loss: 0 .1160 - val_out_2_loss: 0 .0156 - val_out_1_mse: 0 .0013 - val_out_2_mse: 66 .8437 Training: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 1000 /1000 ETA: 00 :00s, 42 .66epochs/sDeepCV:INFO >>> Congrats! Training model is completed. DeepCV:INFO >>> Model has been saved to /home/rketka/github/deepcv/output/model.h5 DeepCV:INFO >>> Weights of model have been saved to /home/rketka/github/deepcv/output/model_weights.h5 DeepCV:INFO >>> Weights of model have been saved to /home/rketka/github/deepcv/output/model_weights.npz DeepCV:INFO >>> Biases of model have been saved to /home/rketka/github/deepcv/output/model_biases.npz DeepCV:INFO >>> Directed graphs of all model have been saved to /mnt/c/Users/Nutt/Desktop/daenn-test DeepCV:INFO >>> Loss history plot has been saved to /home/rketka/github/deepcv/output/loss.png DeepCV:INFO >>> Metric accuracy history plot has been saved to /home/rketka/github/deepcv/output/metrics.png ============================== DONE ============================== Once you see the line \"=== DONE===\" the training is completed. You can then use the output saved in the folder you specified in the input file for further work, i.e., generating CVs.","title":"Models"},{"location":"usage/training-model/#training-a-daenn-model","text":"Note: the current version of DeepCV accepts only datasets that are in NumPy's compressed file formats (.npz).","title":"Training a DAENN model"},{"location":"usage/training-model/#prepare-input-file-for-diels-alder-reaction","text":"DeepCV's input file must be in a JSON file format (dictionary-like). An example of DAENN input in inputs/ folder shows the configuration for training a model using DAENN with five hidden layers. The first three hidden layers contain two encoded layers and one latent encoded layer (middle layer). The rest layers are two decoded layers for reconstruction. On the other hand, the size of two hidden layers that are opposite of each other, e.g., input and output layers, 1 st and 5 th hidden layers, must be the same.","title":"Prepare input file for Diels-Alder reaction"},{"location":"usage/training-model/#call-deepcvs-module","text":"All DeepCV's modules can be called via main.py API script. $ cd deepcv/src/ $ python main.py ------------------------------------------------ DeepCV : Deep Learning for Collective Variables ------------------------------------------------- version 1 .0 : February 2022 University of Zurich, Switzerland https://gitlab.uzh.ch/lubergroup/deepcv Module Description ------------- ---------------------------------------------- calc_rep Calculate molecular representation gen_input Neural network input generator single_train Single-data fully-connected neural network multi_train Multi-data fully-connected neural network daenn Deep autoencoder neural network gan_train Training generative adversarial network ( GAN ) gan_predict Generating samples using trained GAN deepcv2plumed Create PLUMED input file analyze_FES FES validation analyze_model DAENN model analysis and parameters extraction explor_abi Calculate exploration ability For more detail, please review 'README' in the repository.","title":"Call DeepCV's module"},{"location":"usage/training-model/#train-model","text":"Execute the main.py with argument daenn , like below. Then it will start to train the model. The training time depends on the size of the dataset and networks, the number of epochs, etc. $ python main.py daenn -i input/input_ae_DA.json ============================== Program started ============================== Project: Demo === Shape of dataset before splitting === >>> 1 . Dataset: ( 99000 , 15 ) >>> 2 . Dataset: ( 99000 , 14 ) >>> 3 . Dataset: ( 99000 , 13 ) === Shape of dataset after splitting === >>> 1 . Train: ( 79200 , 15 ) & Test: ( 19800 , 15 ) >>> 2 . Train: ( 79200 , 14 ) & Test: ( 19800 , 14 ) >>> 3 . Train: ( 79200 , 13 ) & Test: ( 19800 , 13 ) ... ... ... 1 /1 [==============================] - ETA: 0s - loss: 0 .1283 - out_1_loss: 0 .1268 - out_2_loss: 0 .0154 - out_1_mse: 4 .0202e-04 - out_2_mse: 61 /1\u2588 ETA: 00 :00s - loss: 0 .1283 - out_1_loss: 0 .1268 - out_2_loss: 0 .0154 - out_1_mse: 0 .0004 - out_2_mse: 67 .5435 - val_loss: 0 .1176 - val_ou1/1 [==============================] - 0s 21ms/step - loss: 0 .1283 - out_1_loss: 0 .1268 - out_2_loss: 0 .0154 - out_1_mse: 4 .0202e-04 - out_2_mse: 67 .5435 - val_loss: 0 .1176 - val_out_1_loss: 0 .1160 - val_out_2_loss: 0 .0156 - val_out_1_mse: 0 .0013 - val_out_2_mse: 66 .8437 Training: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 1000 /1000 ETA: 00 :00s, 42 .66epochs/sDeepCV:INFO >>> Congrats! Training model is completed. DeepCV:INFO >>> Model has been saved to /home/rketka/github/deepcv/output/model.h5 DeepCV:INFO >>> Weights of model have been saved to /home/rketka/github/deepcv/output/model_weights.h5 DeepCV:INFO >>> Weights of model have been saved to /home/rketka/github/deepcv/output/model_weights.npz DeepCV:INFO >>> Biases of model have been saved to /home/rketka/github/deepcv/output/model_biases.npz DeepCV:INFO >>> Directed graphs of all model have been saved to /mnt/c/Users/Nutt/Desktop/daenn-test DeepCV:INFO >>> Loss history plot has been saved to /home/rketka/github/deepcv/output/loss.png DeepCV:INFO >>> Metric accuracy history plot has been saved to /home/rketka/github/deepcv/output/metrics.png ============================== DONE ============================== Once you see the line \"=== DONE===\" the training is completed. You can then use the output saved in the folder you specified in the input file for further work, i.e., generating CVs.","title":"Train model"}]}